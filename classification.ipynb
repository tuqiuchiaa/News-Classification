{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import time\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import re, string, timeit\r\n",
    "from nltk.tag.stanford import StanfordNERTagger\r\n",
    "import pickle\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import nltk\r\n",
    "from scipy.sparse import hstack\r\n",
    "pd.set_option('mode.chained_assignment',None)\r\n",
    "import itertools\r\n",
    "\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "from textblob import TextBlob\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_pickle('data/final_Processed_df.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           authors       date  \\\n",
       "0  Melissa Jeltsen 2018-05-26   \n",
       "1    Andy McDonald 2018-05-26   \n",
       "2       Ron Dicker 2018-05-26   \n",
       "3       Ron Dicker 2018-05-26   \n",
       "4       Ron Dicker 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                             NERtext  \\\n",
       "0  There Were CARDINAL Mass Shootings In GPE DATE...   \n",
       "1  Will Smith Joins Diplo And PERSON For WORK_OF_...   \n",
       "2  PERSON For The ORDINAL Time At DATE The actor ...   \n",
       "3  PERSON Blasts GPE PERSON And NORP In New Artwo...   \n",
       "4  PERSON Uses PERSON Poop Bags To Pick Up After ...   \n",
       "\n",
       "                                                text       category  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week...          CRIME  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  ENTERTAINMENT  \n",
       "2  Hugh Grant Marries For The First Time At Age 5...  ENTERTAINMENT  \n",
       "3  Jim Carrey Blasts Castrato Adam Schiff And Dem...  ENTERTAINMENT  \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  ENTERTAINMENT  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>NERtext</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were CARDINAL Mass Shootings In GPE DATE...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And PERSON For WORK_OF_...</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>PERSON For The ORDINAL Time At DATE The actor ...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>PERSON Blasts GPE PERSON And NORP In New Artwo...</td>\n",
       "      <td>Jim Carrey Blasts Castrato Adam Schiff And Dem...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>PERSON Uses PERSON Poop Bags To Pick Up After ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for i in df['text'].head():\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There Were 2 Mass Shootings In Texas Last Week But Only 1 On TV She left her husband He killed their children Just another day in America\n",
      "Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup 's Official Song Of course it has a song\n",
      "Hugh Grant Marries For The First Time At Age 57 The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony\n",
      "Jim Carrey Blasts Castrato Adam Schiff And Democrats In New Artwork The actor gives Dems an ass-kicking for not fighting hard enough against Donald Trump\n",
      "Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog The Dietland actress said using the bags is a really cathartic therapeutic moment\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "for i in df['NERtext'].head():\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There Were CARDINAL Mass Shootings In GPE DATE But DATE On TV She left her husband He killed their children Just another day in GPE\n",
      "Will Smith Joins Diplo And PERSON For WORK_OF_ART Of course it has a song\n",
      "PERSON For The ORDINAL Time At DATE The actor and his longtime girlfriend PERSON tied the knot in a civil ceremony\n",
      "PERSON Blasts GPE PERSON And NORP In New Artwork The actor gives NORP an ass-kicking for not fighting hard enough against PERSON\n",
      "PERSON Uses PERSON Poop Bags To Pick Up After Her Dog The NORP actress said using the bags is a really cathartic therapeutic moment\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "###X_train, X_test, y_train, y_test  =train_test_split(df['text'], df['category'], test_size=0.2, random_state=1, stratify=df['category'])\r\n",
    "#X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=1, stratify=y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class model():\r\n",
    "    def __init__(self, df, textColumn = 'text', SEED = 2):\r\n",
    "        random.seed(SEED)\r\n",
    "        self.SEED = SEED\r\n",
    "        self.df = df\r\n",
    "\r\n",
    "        #text column is whether we use NERtext or normal text column\r\n",
    "        self.textColumn = textColumn\r\n",
    "        self.le = LabelEncoder()\r\n",
    "        self.df['category_code'] = self.le.fit_transform(df['category'])\r\n",
    "\r\n",
    "        \r\n",
    "    def lem(self, phrase, customized__stoppedWords):\r\n",
    "        if customized__stoppedWords:\r\n",
    "            stop_words_ = ['a', 'an', 'the', 'for', 'in', 'it', 'its', 'on', 'at', 'they', 'them',\r\n",
    "                  'their', 'theirs', 'that', 'what', 'which', 'has', 'have', 'had',\r\n",
    "                  'having', 'thus', 'do', 'no', 'nor', 'not', 'is', 'are', 'was',\r\n",
    "                  'were', 'be', 'been', 'being', 'did', 'in', 'out', 'both', 'each',\r\n",
    "                  'few', 'he', 'she', 'him', 'her', 'these', 'those', 'but', 'if',\r\n",
    "                  'of', 'after', 'before', 'up', 'down', 'over', 'under',\r\n",
    "                  'again', 'then', 'there', 'some', 'such', 'other',\r\n",
    "                  'only', 'same', 'so', 'than', 'and', 'to']\r\n",
    "        else:\r\n",
    "            stop_words_ = list(set(stopwords.words('english')))\r\n",
    "        wnl=WordNetLemmatizer()\r\n",
    "        lemmed=[]\r\n",
    "        for i in word_tokenize(phrase):\r\n",
    "            w1=wnl.lemmatize(i, 'v')\r\n",
    "            w2=wnl.lemmatize(w1, 'a')\r\n",
    "            w3=wnl.lemmatize(w2, 'n')\r\n",
    "            if i not in (stop_words_):\r\n",
    "                lemmed.append(w3.lower())\r\n",
    "        return \" \".join(lemmed)\r\n",
    "\r\n",
    "\r\n",
    "        #return \" \".join([wnl.lemmatize(words.lower(), pos=\"v\") for words in word_tokenize(phrase) if (words not in (stop_words_))])\r\n",
    "    def add_sentiment(self):\r\n",
    "        Polarity=[]\r\n",
    "        Subjectivity=[]\r\n",
    "        for i in range(len(self.df[self.textColumn])):\r\n",
    "            x = TextBlob(\" \".join(word_tokenize(self.df[self.textColumn][i]))).sentiment\r\n",
    "            Polarity.append(x[0])\r\n",
    "            Subjectivity.append(x[1])\r\n",
    "        self.df['Polarity'] = Polarity\r\n",
    "        self.df['Subjectivity'] = Subjectivity\r\n",
    "    def add_wordCount(self):\r\n",
    "        self.df['countLen']  = [len(word_tokenize(i)) for i in df[self.textColumn]]\r\n",
    "\r\n",
    "    def fit(self, addAuthor = False, customized__stoppedWords= False, add_sentiment = False, add_wordLen = False):\r\n",
    "        print('-----------Sample Text---------')\r\n",
    "        for i in self.df[self.textColumn].head(3):\r\n",
    "            print(i)\r\n",
    "        self.df[self.textColumn] = self.df[self.textColumn].apply(lambda x: self.lem(x, customized__stoppedWords))\r\n",
    "\r\n",
    "\r\n",
    "        if addAuthor:\r\n",
    "            self.df[self.textColumn] = self.df[self.textColumn] +' ' + self.df['authors']\r\n",
    "\r\n",
    "\r\n",
    "        print('-----------Sample Text---------')\r\n",
    "        for i in self.df[self.textColumn].head(3):\r\n",
    "            print(i)\r\n",
    "        \r\n",
    "        #self.add_sentiment()\r\n",
    "        #self.add_wordCount()\r\n",
    "        '''\r\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test  =train_test_split(self.df[self.textColumn], self.df['category_code'], test_size=0.2, random_state=self.SEED, stratify=self.df['category_code'])\r\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val  = train_test_split(self.X_train, self.y_train, test_size=0.25, random_state=1, stratify=self.y_train)\r\n",
    "        \r\n",
    "        \r\n",
    "        vect_word = TfidfVectorizer(max_features=60000, lowercase=False, analyzer='word',tokenizer=word_tokenize,ngram_range=(1,3),dtype=np.float32)\r\n",
    "        self.tr_vect = vect_word.fit_transform(self.X_train)\r\n",
    "        self.vl_vect = vect_word.transform(self.X_val)\r\n",
    "        \r\n",
    "        print(self.tr_vect.shape[1] , 'total Width of the train vector' )\r\n",
    "        if add_sentiment: \r\n",
    "            self.tr_vect = hstack((self.tr_vect, self.X_train[['Polarity','Subjectivity']].values),format='csr')\r\n",
    "            self.vl_vect = hstack((self.vl_vect, self.X_val[['Polarity','Subjectivity']].values),format='csr')\r\n",
    "        if add_wordLen: \r\n",
    "            self.tr_vect = hstack((self.tr_vect, self.X_train[['countLen']].values),format='csr')\r\n",
    "            self.vl_vect = hstack((self.vl_vect, self.X_val[['countLen']].values),format='csr')\r\n",
    "        print(self.tr_vect.shape[1] , 'total Width of the train vector' )\r\n",
    "        \r\n",
    "        \r\n",
    "        #test_vect = vect_word.transform(X_test)\r\n",
    "        \r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        #logistic regression\r\n",
    "        print('Fitting LGR-------')\r\n",
    "        t = time.process_time()\r\n",
    "        self.lgr = LogisticRegression(solver='saga',multi_class='multinomial', max_iter=1000, C=1,verbose = True, n_jobs = -1, random_state = self.SEED).fit(self.tr_vect, self.y_train)\r\n",
    "        self.lgr_y_pred = self.lgr.predict(self.vl_vect)\r\n",
    "        elapsed_time = time.process_time() - t\r\n",
    "        print('total elappsed time: ', elapsed_time)\r\n",
    "        print('LGR accuracy: ', (self.lgr_y_pred == self.y_val).sum()/len(self.y_val))\r\n",
    "        \r\n",
    "        #print(classification_report(np.asarray(self.le.inverse_transform(self.y_val)),np.asarray( self.le.inverse_transform(self.lgr_y_pred))))\r\n",
    "\r\n",
    "\r\n",
    "        #linear SVC\r\n",
    "        t = time.process_time()\r\n",
    "        print('Fitting LinearSVC-------')\r\n",
    "        self.svc = LinearSVC(verbose = True, random_state = self.SEED).fit(self.tr_vect, self.y_train)\r\n",
    "        self.svc_y_pred = self.svc.predict(self.vl_vect)\r\n",
    "        print('total elappsed time: ', elapsed_time)\r\n",
    "        print('LinearSVC: ', (self.svc_y_pred == self.y_val).sum()/len(self.y_val))\r\n",
    "\r\n",
    "        #print(classification_report(np.asarray(self.le.inverse_transform(self.y_val)),np.asarray( self.le.inverse_transform(self.svc_y_pred))))\r\n",
    "        '''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'NERtext'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "there be cardinal mass shoot in gpe date but date on tv she leave husband he kill children just another day gpe melissa jeltsen\n",
      "will smith join diplo and person for work_of_art of course song andy mcdonald\n",
      "person for the ordinal time at date the actor longtime girlfriend person tie knot civil ceremony ron dicker\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 22 epochs took 34 seconds\n",
      "LGR accuracy: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   34.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 0.7082721366159668\n",
      "[LibLinear]LinearSVC:  0.7256229618381419\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "textColumnToUse = 'text'\r\n",
    "model = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "model.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "there be 2 mass shoot in texas last week but only 1 on tv she leave husband he kill children just another day america melissa jeltsen\n",
      "will smith join diplo and nicky jam for the 2018 world cup 's official song of course song andy mcdonald\n",
      "hugh grant marry for the first time at age 57 the actor longtime girlfriend anna eberstein tie knot civil ceremony ron dicker\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 19 epochs took 29 seconds\n",
      "LGR accuracy:  0.7315974210251176\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]LinearSVC:  0.7522839859600209\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'text'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit(customized__stoppedWords= True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "2 mass shoot texas last week 1 tv leave husband kill child just another day america melissa jeltsen Melissa Jeltsen\n",
      "will smith join diplo nicky jam 2018 world cup 's official song course song andy mcdonald Andy McDonald\n",
      "hugh grant marry first time age 57 actor longtime girlfriend anna eberstein tie knot civil ceremony ron dicker Ron Dicker\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 21 epochs took 29 seconds\n",
      "LGR accuracy:  0.7399118767269921\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   28.7s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]LinearSVC:  0.755993129371935\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "wnl=WordNetLemmatizer()\r\n",
    "lemmed=[]\r\n",
    "for i in word_tokenize(df['text'][0]+' ' + df['authors'][0]):\r\n",
    "    w1=wnl.lemmatize(i, 'v')\r\n",
    "    w2=wnl.lemmatize(w1, 'a')\r\n",
    "    w3=wnl.lemmatize(w2, 'n')\r\n",
    "    \r\n",
    "    lemmed.append(w3.lower())\r\n",
    "    print('lemmed', w3)\r\n",
    "print(\" \".join(lemmed))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lemmed There\n",
      "lemmed Were\n",
      "lemmed 2\n",
      "lemmed Mass\n",
      "lemmed Shootings\n",
      "lemmed In\n",
      "lemmed Texas\n",
      "lemmed Last\n",
      "lemmed Week\n",
      "lemmed But\n",
      "lemmed Only\n",
      "lemmed 1\n",
      "lemmed On\n",
      "lemmed TV\n",
      "lemmed She\n",
      "lemmed leave\n",
      "lemmed her\n",
      "lemmed husband\n",
      "lemmed He\n",
      "lemmed kill\n",
      "lemmed their\n",
      "lemmed child\n",
      "lemmed Just\n",
      "lemmed another\n",
      "lemmed day\n",
      "lemmed in\n",
      "lemmed America\n",
      "lemmed Melissa\n",
      "lemmed Jeltsen\n",
      "there were 2 mass shootings in texas last week but only 1 on tv she leave her husband he kill their child just another day in america melissa jeltsen\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'text'\r\n",
    "addAuthor = True\r\n",
    "model1 = model(df.head(5), textColumnToUse, addAuthor, SEED)\r\n",
    "model1.fit(customized__stoppedWords= True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "there were 2 mass shootings in texas last week but only 1 on tv she leave husband he kill child just another day america melissa jeltsen Melissa Jeltsen\n",
      "will smith joins diplo and nicky jam for the 2018 world cup 's official song of course song andy mcdonald Andy McDonald\n",
      "hugh grant marries for the first time at age 57 the actor his longtime girlfriend anna eberstein tie knot civil ceremony ron dicker Ron Dicker\n",
      "-----------Sample Text---------\n",
      "2 mass shoot texas last week 1 tv leave husband kill child just another day america melissa jeltsen melissa jeltsen\n",
      "will smith join diplo nicky jam 2018 world cup 's official song course song andy mcdonald andy mcdonald\n",
      "hugh grant marry first time age 57 actor his longtime girlfriend anna eberstein tie knot civil ceremony ron dicker ron dicker\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "report = classification_report(np.asarray(model1.le.inverse_transform(model1.y_val)),np.asarray( model1.le.inverse_transform(model1.svc_y_pred)), output_dict=True)\r\n",
    "report_df = pd.DataFrame(report)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "report_df.transpose().to_csv('data/report.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'NERtext'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit(customized__stoppedWords= True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "There Were CARDINAL Mass Shootings In GPE DATE But DATE On TV She leave husband He kill child Just another day GPE Melissa Jeltsen Melissa Jeltsen\n",
      "Will Smith Joins Diplo And PERSON For WORK_OF_ART Of course song Andy McDonald Andy McDonald\n",
      "PERSON For The ORDINAL Time At DATE The actor his longtime girlfriend PERSON tie knot civil ceremony Ron Dicker Ron Dicker\n",
      "60000 total Width of the train vector\n",
      "60000 total Width of the train vector\n",
      "Fitting LGR-------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 21 epochs took 29 seconds\n",
      "total elappsed time:  27.953125\n",
      "LGR accuracy:  0.7070772447785716\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   28.6s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.75      0.42      0.54       302\n",
      "ARTS & CULTURE       0.65      0.54      0.59       268\n",
      "  BLACK VOICES       0.73      0.47      0.57       905\n",
      "      BUSINESS       0.61      0.57      0.59      1188\n",
      "       COLLEGE       0.72      0.40      0.52       229\n",
      "        COMEDY       0.61      0.58      0.60      1035\n",
      "         CRIME       0.62      0.54      0.58       681\n",
      "CULTURE & ARTS       0.93      0.32      0.48       206\n",
      "       DIVORCE       0.89      0.68      0.78       685\n",
      "     EDUCATION       0.75      0.35      0.48       201\n",
      " ENTERTAINMENT       0.69      0.81      0.74      3212\n",
      "   ENVIRONMENT       0.88      0.29      0.44       264\n",
      "         FIFTY       0.81      0.44      0.57       280\n",
      "  FOOD & DRINK       0.86      0.80      0.83      1245\n",
      "     GOOD NEWS       0.78      0.47      0.58       279\n",
      "         GREEN       0.60      0.35      0.44       524\n",
      "HEALTHY LIVING       0.61      0.68      0.64      1339\n",
      " HOME & LIVING       0.94      0.79      0.86       839\n",
      "        IMPACT       0.59      0.40      0.47       692\n",
      " LATINO VOICES       0.91      0.46      0.62       226\n",
      "         MEDIA       0.79      0.42      0.55       563\n",
      "         MONEY       0.84      0.50      0.62       341\n",
      "     PARENTING       0.69      0.79      0.74      1736\n",
      "       PARENTS       0.72      0.62      0.67       791\n",
      "      POLITICS       0.67      0.91      0.77      6548\n",
      "  QUEER VOICES       0.86      0.70      0.77      1263\n",
      "      RELIGION       0.70      0.53      0.60       511\n",
      "       SCIENCE       0.83      0.49      0.61       435\n",
      "        SPORTS       0.77      0.61      0.68       977\n",
      "         STYLE       0.73      0.55      0.63       451\n",
      "STYLE & BEAUTY       0.84      0.86      0.85      1930\n",
      "         TASTE       0.82      0.50      0.62       419\n",
      "          TECH       0.83      0.47      0.60       416\n",
      " THE WORLDPOST       0.54      0.51      0.53       733\n",
      "        TRAVEL       0.80      0.86      0.83      1978\n",
      "      WEDDINGS       0.90      0.72      0.80       730\n",
      "    WEIRD NEWS       0.50      0.57      0.54       534\n",
      "      WELLNESS       0.67      0.90      0.77      3566\n",
      "         WOMEN       0.63      0.48      0.54       698\n",
      "    WORLD NEWS       0.48      0.22      0.30       435\n",
      "     WORLDPOST       0.62      0.43      0.51       516\n",
      "\n",
      "      accuracy                           0.71     40171\n",
      "     macro avg       0.74      0.56      0.62     40171\n",
      "  weighted avg       0.72      0.71      0.70     40171\n",
      "\n",
      "Fitting LinearSVC-------\n",
      "[LibLinear]total elappsed time:  27.953125\n",
      "LinearSVC:  0.7304025291877225\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.68      0.53      0.59       302\n",
      "ARTS & CULTURE       0.67      0.56      0.61       268\n",
      "  BLACK VOICES       0.62      0.55      0.59       905\n",
      "      BUSINESS       0.59      0.59      0.59      1188\n",
      "       COLLEGE       0.63      0.53      0.57       229\n",
      "        COMEDY       0.61      0.60      0.61      1035\n",
      "         CRIME       0.64      0.60      0.62       681\n",
      "CULTURE & ARTS       0.79      0.48      0.59       206\n",
      "       DIVORCE       0.84      0.77      0.80       685\n",
      "     EDUCATION       0.66      0.48      0.55       201\n",
      " ENTERTAINMENT       0.73      0.80      0.76      3212\n",
      "   ENVIRONMENT       0.68      0.43      0.53       264\n",
      "         FIFTY       0.78      0.65      0.71       280\n",
      "  FOOD & DRINK       0.86      0.83      0.84      1245\n",
      "     GOOD NEWS       0.64      0.53      0.58       279\n",
      "         GREEN       0.55      0.48      0.51       524\n",
      "HEALTHY LIVING       0.68      0.67      0.68      1339\n",
      " HOME & LIVING       0.87      0.83      0.85       839\n",
      "        IMPACT       0.55      0.47      0.51       692\n",
      " LATINO VOICES       0.78      0.54      0.64       226\n",
      "         MEDIA       0.69      0.51      0.59       563\n",
      "         MONEY       0.75      0.60      0.67       341\n",
      "     PARENTING       0.75      0.81      0.78      1736\n",
      "       PARENTS       0.72      0.67      0.69       791\n",
      "      POLITICS       0.76      0.87      0.81      6548\n",
      "  QUEER VOICES       0.83      0.76      0.80      1263\n",
      "      RELIGION       0.65      0.62      0.64       511\n",
      "       SCIENCE       0.74      0.63      0.68       435\n",
      "        SPORTS       0.73      0.68      0.70       977\n",
      "         STYLE       0.70      0.65      0.68       451\n",
      "STYLE & BEAUTY       0.85      0.87      0.86      1930\n",
      "         TASTE       0.73      0.57      0.64       419\n",
      "          TECH       0.70      0.56      0.62       416\n",
      " THE WORLDPOST       0.54      0.50      0.52       733\n",
      "        TRAVEL       0.80      0.87      0.83      1978\n",
      "      WEDDINGS       0.87      0.81      0.84       730\n",
      "    WEIRD NEWS       0.52      0.57      0.54       534\n",
      "      WELLNESS       0.78      0.88      0.83      3566\n",
      "         WOMEN       0.56      0.52      0.54       698\n",
      "    WORLD NEWS       0.40      0.27      0.32       435\n",
      "     WORLDPOST       0.55      0.53      0.54       516\n",
      "\n",
      "      accuracy                           0.73     40171\n",
      "     macro avg       0.69      0.63      0.65     40171\n",
      "  weighted avg       0.73      0.73      0.73     40171\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#Test with subjectivity len and polarity\r\n",
    "'''SEED = 2\r\n",
    "textColumnToUse = 'text'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit(customized__stoppedWords= True,add_sentiment = True, add_wordLen = True)'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"SEED = 2\\ntextColumnToUse = 'text'\\naddAuthor = True\\nnermodel = model(df, textColumnToUse, addAuthor, SEED)\\nnermodel.fit(customized__stoppedWords= True,add_sentiment = True, add_wordLen = True)\""
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#Test with subjectivity len and polarity\r\n",
    "'''SEED = 2\r\n",
    "textColumnToUse = 'NERtext'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit(customized__stoppedWords= True,add_sentiment = True, add_wordLen = True)'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"SEED = 2\\ntextColumnToUse = 'NERtext'\\naddAuthor = True\\nnermodel = model(df, textColumnToUse, addAuthor, SEED)\\nnermodel.fit(customized__stoppedWords= True,add_sentiment = True, add_wordLen = True)\""
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# best model features exam the responses\r\n",
    "\r\n",
    "model1.predit(model1.)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "model1.svc = LinearSVC(verbose = True).fit(model1.tr_vect, model1.y_train)\r\n",
    "model1.svc_y_pred = model1.svc.predict(model1.vl_vect)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "print('LinearSVC: ', (model1.svc_y_pred == model1.y_val).sum()/len(model1.y_val))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LinearSVC:  0.6924896069303726\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'NERtext'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit(customized__stoppedWords= True,add_sentiment = True, add_wordLen = True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[26, 24, 26, 28, 28]"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "##################################################Results######################################\r\n",
    "# without author NER LGR accuracy:  0.5542804510716686 LinearSVC:  0.554429812551343, BERT accuracy: 0.6484048366546631\r\n",
    "# without author LGR accuracy:  0.6009310198899703 LinearSVC:  0.6112618555674492 BERT accuracy: 0.6991676092147827\r\n",
    "\r\n",
    "# with Author NER LGR accuracy:  0.7082721366159668 LinearSVC:   0.7256229618381419, BERT: 0.7828295230865479\r\n",
    "# with Author LGR accuracy:  0.7315974210251176 LinearSVC:  0.7522839859600209, BERT: 0.8054925203323364\r\n",
    "\r\n",
    "#---------tailored stop words--------------------\r\n",
    "# with Author NER LGR aacurac 0.6973687485997361, LinearSVC:  0.7229095616240572\r\n",
    "# with Author LGR aacuracy: 0.7399118767269921 LinearSVC:  0.755993129371935\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df.groupby(by='category').size().sort_values(ascending = False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "category\n",
       "POLITICS          32739\n",
       "WELLNESS          17827\n",
       "ENTERTAINMENT     16058\n",
       "TRAVEL             9887\n",
       "STYLE & BEAUTY     9649\n",
       "PARENTING          8677\n",
       "HEALTHY LIVING     6694\n",
       "QUEER VOICES       6314\n",
       "FOOD & DRINK       6226\n",
       "BUSINESS           5937\n",
       "COMEDY             5175\n",
       "SPORTS             4884\n",
       "BLACK VOICES       4528\n",
       "HOME & LIVING      4195\n",
       "PARENTS            3955\n",
       "THE WORLDPOST      3664\n",
       "WEDDINGS           3651\n",
       "WOMEN              3490\n",
       "IMPACT             3459\n",
       "DIVORCE            3426\n",
       "CRIME              3405\n",
       "MEDIA              2815\n",
       "WEIRD NEWS         2670\n",
       "GREEN              2622\n",
       "WORLDPOST          2579\n",
       "RELIGION           2556\n",
       "STYLE              2254\n",
       "SCIENCE            2178\n",
       "WORLD NEWS         2177\n",
       "TASTE              2096\n",
       "TECH               2082\n",
       "MONEY              1707\n",
       "ARTS               1509\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "ARTS & CULTURE     1339\n",
       "ENVIRONMENT        1323\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1129\n",
       "CULTURE & ARTS     1030\n",
       "EDUCATION          1004\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "test_features = ['TASTE', 'WEDDINGS']\r\n",
    "test_df = df[df['category'].isin(test_features) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "for i in test_df.head()['text']:\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It 's Weird That American Restaurants Serve Ice Water In Winter But why do we even have ice in our drinks in the first place\n",
      "Pineapple Casserole The Southern Dish That 's A Paradox Of Flavors It 's got pineapple cheddar and a whole lot of butter\n",
      "How To Actually Get A Bartender 's Attention Plus other things they wish you knew\n",
      "Diet Coke 's Millennial-Inspired Makeover Leaves People Befuddled It 's not like a regular soda it 's a cool soda\n",
      "We Tested The New Tearless Onions To See If They Really Work Put away your goggles people\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "for i in test_df.head()['NERtext']:\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It 's Weird That American Restaurants Serve Ice Water In Winter But why do we even have ice in our drinks in the ORDINAL place\n",
      "Pineapple Casserole The Southern Dish That 's A Paradox Of Flavors It 's got pineapple cheddar and a whole lot of butter\n",
      "How To Actually Get A Bartender 's Attention Plus other things they wish you knew\n",
      "PERSON Millennial - Inspired Makeover Leaves People Befuddled It 's not like a regular soda it 's a cool soda\n",
      "We Tested The New Tearless Onions To See If They Really Work Put away your goggles people\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'NERtext'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(test_df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "it 's weird that american restaurants serve ice water in winter but even ice drink ordinal place todd van luling\n",
      "pineapple casserole the southern dish that 's a paradox of flavor it 's get pineapple cheddar whole lot butter kristen aiken\n",
      "how to actually get a bartender 's attention plus things wish know taylor pittman\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 20 epochs took 0 seconds\n",
      "LGR accuracy:  0.9834782608695652\n",
      "[LibLinear]LinearSVC:  0.9904347826086957\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'text'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(test_df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "it 's weird that american restaurants serve ice water in winter but even ice drink first place todd van luling\n",
      "pineapple casserole the southern dish that 's a paradox of flavor it 's get pineapple cheddar whole lot butter kristen aiken\n",
      "how to actually get a bartender 's attention plus things wish know taylor pittman\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 21 epochs took 0 seconds\n",
      "LGR accuracy:  0.9904347826086957\n",
      "[LibLinear]LinearSVC:  0.9947826086956522\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}