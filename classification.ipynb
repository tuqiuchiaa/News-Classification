{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import time\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import re, string, timeit\r\n",
    "from nltk.tag.stanford import StanfordNERTagger\r\n",
    "import pickle\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import nltk\r\n",
    "pd.set_option('mode.chained_assignment',None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\r\n",
    "from sklearn.svm import LinearSVC\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import tensorflow as tf\r\n",
    "import transformers #huggingface transformers library\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df = pd.read_pickle('data/final_Processed_df.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           authors       date  \\\n",
       "0  Melissa Jeltsen 2018-05-26   \n",
       "1    Andy McDonald 2018-05-26   \n",
       "2       Ron Dicker 2018-05-26   \n",
       "3       Ron Dicker 2018-05-26   \n",
       "4       Ron Dicker 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                             NERtext  \\\n",
       "0  There Were CARDINAL Mass Shootings In GPE DATE...   \n",
       "1  Will Smith Joins Diplo And PERSON For WORK_OF_...   \n",
       "2  PERSON For The ORDINAL Time At DATE The actor ...   \n",
       "3  PERSON Blasts GPE PERSON And NORP In New Artwo...   \n",
       "4  PERSON Uses PERSON Poop Bags To Pick Up After ...   \n",
       "\n",
       "                                                text       category  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week...          CRIME  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  ENTERTAINMENT  \n",
       "2  Hugh Grant Marries For The First Time At Age 5...  ENTERTAINMENT  \n",
       "3  Jim Carrey Blasts Castrato Adam Schiff And Dem...  ENTERTAINMENT  \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  ENTERTAINMENT  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>NERtext</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were CARDINAL Mass Shootings In GPE DATE...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And PERSON For WORK_OF_...</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>PERSON For The ORDINAL Time At DATE The actor ...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>PERSON Blasts GPE PERSON And NORP In New Artwo...</td>\n",
       "      <td>Jim Carrey Blasts Castrato Adam Schiff And Dem...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>PERSON Uses PERSON Poop Bags To Pick Up After ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for i in df['text'].head():\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There Were 2 Mass Shootings In Texas Last Week But Only 1 On TV She left her husband He killed their children Just another day in America\n",
      "Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup 's Official Song Of course it has a song\n",
      "Hugh Grant Marries For The First Time At Age 57 The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony\n",
      "Jim Carrey Blasts Castrato Adam Schiff And Democrats In New Artwork The actor gives Dems an ass-kicking for not fighting hard enough against Donald Trump\n",
      "Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog The Dietland actress said using the bags is a really cathartic therapeutic moment\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\r\n",
    "for i in df['NERtext'].head():\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There Were CARDINAL Mass Shootings In GPE DATE But DATE On TV She left her husband He killed their children Just another day in GPE\n",
      "Will Smith Joins Diplo And PERSON For WORK_OF_ART Of course it has a song\n",
      "PERSON For The ORDINAL Time At DATE The actor and his longtime girlfriend PERSON tied the knot in a civil ceremony\n",
      "PERSON Blasts GPE PERSON And NORP In New Artwork The actor gives NORP an ass-kicking for not fighting hard enough against PERSON\n",
      "PERSON Uses PERSON Poop Bags To Pick Up After Her Dog The NORP actress said using the bags is a really cathartic therapeutic moment\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "###X_train, X_test, y_train, y_test  =train_test_split(df['text'], df['category'], test_size=0.2, random_state=1, stratify=df['category'])\r\n",
    "#X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=1, stratify=y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class model():\r\n",
    "    def __init__(self, df, textColumn = 'text', addAuthor = False, SEED = 2):\r\n",
    "        self.SEED = SEED\r\n",
    "        self.df = df\r\n",
    "        self.textColumn = textColumn\r\n",
    "        self.le = LabelEncoder()\r\n",
    "        self.df['category_code'] = self.le.fit_transform(df['category'])\r\n",
    "\r\n",
    "        if addAuthor:\r\n",
    "            self.df[textColumn] = self.df[textColumn] +' ' + self.df['authors']\r\n",
    "    def lem(self, phrase, rarewords):\r\n",
    "        stop_words_ = list(set(stopwords.words('english')))\r\n",
    "        wn = WordNetLemmatizer()\r\n",
    "        return \" \".join([wn.lemmatize(words.lower(), pos=\"v\") for words in word_tokenize(phrase) if (words not in (stop_words_ + rarewords))])\r\n",
    "\r\n",
    "    def fit(self, rarewords = []):\r\n",
    "        self.df[self.textColumn] = self.df[self.textColumn].apply(lambda x: self.lem(x, rarewords))\r\n",
    "        print('-----------Sample Text---------')\r\n",
    "        for i in self.df[self.textColumn].head(3):\r\n",
    "            print(i)\r\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test  =train_test_split(self.df[self.textColumn], self.df['category_code'], test_size=0.2, random_state=1, stratify=self.df['category_code'])\r\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val  = train_test_split(self.X_train, self.y_train, test_size=0.25, random_state=1, stratify=self.y_train)\r\n",
    "        vect_word = TfidfVectorizer(max_features=60000, lowercase=False, analyzer='word',tokenizer=word_tokenize,ngram_range=(1,3),dtype=np.float32)\r\n",
    "        tr_vect = vect_word.fit_transform(self.X_train)\r\n",
    "        vl_vect = vect_word.transform(self.X_val)\r\n",
    "        #test_vect = vect_word.transform(X_test)\r\n",
    "        \r\n",
    "        #logistic regression\r\n",
    "        self.lgr = LogisticRegression(solver='saga',multi_class='multinomial', max_iter=1000, C=1,verbose = True, n_jobs = -1, random_state = self.SEED).fit(tr_vect, self.y_train)\r\n",
    "        self.lgr_y_pred = self.lgr.predict(vl_vect)\r\n",
    "        print('LGR accuracy: ', (self.lgr_y_pred == self.y_val).sum()/len(self.y_val))\r\n",
    "\r\n",
    "        #linear SVC\r\n",
    "        \r\n",
    "        self.svc = LinearSVC(verbose = True, random_state = self.SEED).fit(tr_vect, self.y_train)\r\n",
    "        self.svc_y_pred = self.svc.predict(vl_vect)\r\n",
    "        print('LinearSVC: ', (self.svc_y_pred == self.y_val).sum()/len(self.y_val))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'NERtext'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "there be cardinal mass shoot in gpe date but date on tv she leave husband he kill children just another day gpe melissa jeltsen\n",
      "will smith join diplo and person for work_of_art of course song andy mcdonald\n",
      "person for the ordinal time at date the actor longtime girlfriend person tie knot civil ceremony ron dicker\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 22 epochs took 34 seconds\n",
      "LGR accuracy: "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   34.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 0.7082721366159668\n",
      "[LibLinear]LinearSVC:  0.7256229618381419\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "textColumnToUse = 'text'\r\n",
    "model = model(df, textColumnToUse, addAuthor)\r\n",
    "model.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "there be 2 mass shoot in texas last week but only 1 on tv she leave husband he kill children just another day america melissa jeltsen\n",
      "will smith join diplo and nicky jam for the 2018 world cup 's official song of course song andy mcdonald\n",
      "hugh grant marry for the first time at age 57 the actor longtime girlfriend anna eberstein tie knot civil ceremony ron dicker\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 19 epochs took 29 seconds\n",
      "LGR accuracy:  0.7315974210251176\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]LinearSVC:  0.7522839859600209\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# without author NER LGR accuracy:  0.5542804510716686 LinearSVC:  0.554429812551343, BERT accuracy: 0.6484048366546631\r\n",
    "# without author LGR accuracy:  0.6009310198899703 LinearSVC:  0.6112618555674492 BERT accuracy: 0.6991676092147827\r\n",
    "\r\n",
    "# with Author NER LGR accuracy:  0.7082721366159668 LinearSVC:   0.7256229618381419\r\n",
    "# with Author LGR accuracy:  0.7315974210251176 LinearSVC:  0.7522839859600209\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df.groupby(by='category').size().sort_values(ascending = False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "category\n",
       "POLITICS          32739\n",
       "WELLNESS          17827\n",
       "ENTERTAINMENT     16058\n",
       "TRAVEL             9887\n",
       "STYLE & BEAUTY     9649\n",
       "PARENTING          8677\n",
       "HEALTHY LIVING     6694\n",
       "QUEER VOICES       6314\n",
       "FOOD & DRINK       6226\n",
       "BUSINESS           5937\n",
       "COMEDY             5175\n",
       "SPORTS             4884\n",
       "BLACK VOICES       4528\n",
       "HOME & LIVING      4195\n",
       "PARENTS            3955\n",
       "THE WORLDPOST      3664\n",
       "WEDDINGS           3651\n",
       "WOMEN              3490\n",
       "IMPACT             3459\n",
       "DIVORCE            3426\n",
       "CRIME              3405\n",
       "MEDIA              2815\n",
       "WEIRD NEWS         2670\n",
       "GREEN              2622\n",
       "WORLDPOST          2579\n",
       "RELIGION           2556\n",
       "STYLE              2254\n",
       "SCIENCE            2178\n",
       "WORLD NEWS         2177\n",
       "TASTE              2096\n",
       "TECH               2082\n",
       "MONEY              1707\n",
       "ARTS               1509\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "ARTS & CULTURE     1339\n",
       "ENVIRONMENT        1323\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1129\n",
       "CULTURE & ARTS     1030\n",
       "EDUCATION          1004\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "test_features = ['TASTE', 'WEDDINGS']\r\n",
    "test_df = df[df['category'].isin(test_features) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "for i in test_df.head()['text']:\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It 's Weird That American Restaurants Serve Ice Water In Winter But why do we even have ice in our drinks in the first place\n",
      "Pineapple Casserole The Southern Dish That 's A Paradox Of Flavors It 's got pineapple cheddar and a whole lot of butter\n",
      "How To Actually Get A Bartender 's Attention Plus other things they wish you knew\n",
      "Diet Coke 's Millennial-Inspired Makeover Leaves People Befuddled It 's not like a regular soda it 's a cool soda\n",
      "We Tested The New Tearless Onions To See If They Really Work Put away your goggles people\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "for i in test_df.head()['NERtext']:\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It 's Weird That American Restaurants Serve Ice Water In Winter But why do we even have ice in our drinks in the ORDINAL place\n",
      "Pineapple Casserole The Southern Dish That 's A Paradox Of Flavors It 's got pineapple cheddar and a whole lot of butter\n",
      "How To Actually Get A Bartender 's Attention Plus other things they wish you knew\n",
      "PERSON Millennial - Inspired Makeover Leaves People Befuddled It 's not like a regular soda it 's a cool soda\n",
      "We Tested The New Tearless Onions To See If They Really Work Put away your goggles people\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'NERtext'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(test_df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "it 's weird that american restaurants serve ice water in winter but even ice drink ordinal place todd van luling\n",
      "pineapple casserole the southern dish that 's a paradox of flavor it 's get pineapple cheddar whole lot butter kristen aiken\n",
      "how to actually get a bartender 's attention plus things wish know taylor pittman\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 20 epochs took 0 seconds\n",
      "LGR accuracy:  0.9834782608695652\n",
      "[LibLinear]LinearSVC:  0.9904347826086957\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "SEED = 2\r\n",
    "textColumnToUse = 'text'\r\n",
    "addAuthor = True\r\n",
    "nermodel = model(test_df, textColumnToUse, addAuthor, SEED)\r\n",
    "nermodel.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "it 's weird that american restaurants serve ice water in winter but even ice drink first place todd van luling\n",
      "pineapple casserole the southern dish that 's a paradox of flavor it 's get pineapple cheddar whole lot butter kristen aiken\n",
      "how to actually get a bartender 's attention plus things wish know taylor pittman\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 21 epochs took 0 seconds\n",
      "LGR accuracy:  0.9904347826086957\n",
      "[LibLinear]LinearSVC:  0.9947826086956522\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "stop_words_ = set(stopwords.words('english'))\r\n",
    "wn = WordNetLemmatizer()\r\n",
    "def lem(phrase):\r\n",
    "        return \" \".join([wn.lemmatize(words.lower(), pos=\"v\") for words in word_tokenize(phrase) if (words not in stop_words_)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "df_lem = df.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "df_lem['text'] = df['text'].apply(lambda x: lem(x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "fulltext = ' '.join(df_lem['text'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "wordFreq = nltk.FreqDist(word_tokenize(fulltext))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "wordFreq"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FreqDist({'the': 82658, \"'s\": 79341, 'i': 46365, 'be': 40582, 'to': 40318, 'a': 31798, 'in': 30472, \"n't\": 24366, 'of': 23702, 'for': 21983, ...})"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "rareWords = len([k for k, v in sorted(wordFreq.items(), key=lambda item: item[1]) if v ==1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "textColumnToUse = 'text'\r\n",
    "model = model(df, textColumnToUse, addAuthor,rareWords)\r\n",
    "model.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------Sample Text---------\n",
      "there be 2 mass shoot in texas last week but only 1 on tv she leave husband he kill children just another day america melissa jeltsen melissa jeltsen\n",
      "will smith join diplo and nicky jam for the 2018 world cup 's official song of course song andy mcdonald andy mcdonald\n",
      "hugh grant marry for the first time at age 57 the actor longtime girlfriend anna eberstein tie knot civil ceremony ron dicker ron dicker\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convergence after 21 epochs took 27 seconds\n",
      "LGR accuracy:  0.7376216673719848\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   27.6s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]LinearSVC:  0.7584575937865624\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers.recurrent import LSTM, GRU\r\n",
    "from keras.layers.core import Dense, Activation, Dropout\r\n",
    "from keras.layers.embeddings import Embedding\r\n",
    "from keras.layers.normalization import BatchNormalization\r\n",
    "from keras.utils import np_utils\r\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\r\n",
    "from keras.preprocessing import sequence, text\r\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "types = df.category.unique()\r\n",
    "def get_type_index(string):\r\n",
    "    return list(types).index(string)\r\n",
    "df['type_index'] = df['category'].apply(get_type_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train, test = train_test_split(df)\r\n",
    "train, val = train_test_split(train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "\r\n",
    "vocab_size = 10000\r\n",
    "trunc_type = \"post\"\r\n",
    "pad_type = \"post\"\r\n",
    "oov_tok = \"<OOV>\"\r\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\r\n",
    "tokenizer.fit_on_texts(df.text.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "one_hot_labels = tf.keras.utils.to_categorical(train.type_index.values, num_classes=41)\r\n",
    "val_labels= tf.keras.utils.to_categorical(val.type_index.values, num_classes=41)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import transformers\r\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-large-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "maxlen=200\r\n",
    "train_input_ids = [tokenizer.encode(str(i), max_length = maxlen , pad_to_max_length = True) for i in train.text.values]\r\n",
    "val_input_ids = [tokenizer.encode(str(i), max_length = maxlen , pad_to_max_length = True) for i in val.text.values]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def create_model(): \r\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32,\r\n",
    "                                           name=\"input_word_ids\")\r\n",
    "    bert_layer = transformers.TFBertModel.from_pretrained('bert-large-uncased')\r\n",
    "    bert_outputs = bert_layer(input_word_ids)[0]\r\n",
    "    pred = tf.keras.layers.Dense(41, activation='softmax')(bert_outputs[:,0,:])\r\n",
    "    \r\n",
    "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=pred)\r\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(\r\n",
    "    learning_rate=0.00001), metrics=['accuracy'])\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "tf.test.gpu_device_name()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "use_tpu = False\r\n",
    "if use_tpu:\r\n",
    "    # Create distribution strategy\r\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\r\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
    "\r\n",
    "    # Create model\r\n",
    "    with strategy.scope():\r\n",
    "        model = create_model()\r\n",
    "else:\r\n",
    "    model = create_model()\r\n",
    "    \r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel TFBaseModelOutputWithPool 335141888 \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 41)                42025     \n",
      "=================================================================\n",
      "Total params: 335,183,913\n",
      "Trainable params: 335,183,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "batch_size = 64\r\n",
    "\r\n",
    "history=model.fit(np.array(train_input_ids), one_hot_labels,validation_data = (np.array(val_input_ids), val_labels),epochs = 4, batch_size = batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[64,16,200,200] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node model_1/tf_bert_model_1/bert/encoder/layer_._15/attention/self/dropout_119/dropout/random_uniform/RandomUniform (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:272) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_74266]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e6db2f9342ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_input_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,16,200,200] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node model_1/tf_bert_model_1/bert/encoder/layer_._15/attention/self/dropout_119/dropout/random_uniform/RandomUniform (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:272) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_74266]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = [i for i in range(4)]\r\n",
    "fig , ax = plt.subplots(1,2)\r\n",
    "train_acc = history.history['accuracy']\r\n",
    "train_loss = history.history['loss']\r\n",
    "val_acc = history.history['val_accuracy']\r\n",
    "val_loss = history.history['val_loss']\r\n",
    "fig.set_size_inches(20,10)\r\n",
    "\r\n",
    "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\r\n",
    "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\r\n",
    "ax[0].set_title('Training & Testing Accuracy')\r\n",
    "ax[0].legend()\r\n",
    "ax[0].set_xlabel(\"Epochs\")\r\n",
    "ax[0].set_ylabel(\"Accuracy\")\r\n",
    "\r\n",
    "ax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\r\n",
    "ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\r\n",
    "ax[1].set_title('Training & Testing Loss')\r\n",
    "ax[1].legend()\r\n",
    "ax[1].set_xlabel(\"Epochs\")\r\n",
    "ax[1].set_ylabel(\"Loss\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}